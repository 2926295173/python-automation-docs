
> 在开始之前，我们需要安装一些必要的第三方扩展库

```shell
pip install requests
pip install lxml
```
`Requests` 是 `Python` 爬虫中的一把利器，如果你不是选择诸如 `Scrapy` 这样的爬虫框架的话，那么 `Requests` 可能是你最好的选择。
当然还有其他优秀的扩展库。

此文档重点不在 `Python`爬虫领域，而在于自动化相关，所以不会涉及类如 `Scrapy`、`PySpider`、`Celery` 这些框架的的讲解，不用担心 ！！

我们通过一下的例子来初步了解 `Requests` 库的用法，爬虫爱好者们总是酷爱使用`豆瓣`网站来作为他们的第一个目标网站。

我们将通过 [豆瓣-正在热映](https://movie.douban.com/cinema/nowplaying/guangzhou/) 这个页面提取到正在上映电影
的名称，图片，评分，以及详情链接。


## 获取网页源码


代码如下:

```python
啪啪啪 这是代码

```

得到了以下一些输出:
```text
balaaaaaaa
```

可以看到我们成功了打印出了这个页面的 `html` 源码，完成了第一步。

## 解析网页源码

现在我们对已经获取的 `html` 源码进行解析，这里以 `xpath` 为例， 常用的解析库还有`bs4`、`Pyquery`、`css选择器等等`，
有兴趣的读者可以自行搜索并进行比较，目前来说，`xpath`的解析速度是最快的。

```python
这里对豆瓣网页源码进行解析
```

可以看到经过我们的解析之后，提取的数据变得清晰有结构，只提取保留对我们有用的信息。

## 获取 `Json` 结构化数据

